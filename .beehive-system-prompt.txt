================================================================================
PLAN CONTEXT - Your task is part of an ordered plan:
================================================================================

Plan: Scaffold the ml_experiments framework from scratch. This is an empty repo that needs full project setup. Create: (1) pyproject.toml with all dependencies (torch, diffusers, transformers, accelerate, modal, webdataset, boto3, pydantic, rich, bitsandbytes, torchmetrics), (2) core/experiment.py base Experiment class with config loading, metric logging, and checkpointing, (3) core/modal_runner.py with Modal app definition including a CUDA+PyTorch+HF GPU image, modal.Volume for HF cache, and a generic run_experiment function that instantiates and runs any experiment on Modal GPUs, (4) core/data.py with WebDataset S3 streaming utilities — get_dataset() that takes a dataset config with S3 path and returns a streaming WebDataset pipeline with standard image transforms, (5) core/metrics.py with FID, latency, memory profiling utilities, (6) tasks/ with stub experiment classes for text_to_image, matting, segmentation, synthetic_data, vae, text_encoder — each extending base Experiment, (7) benchmarks/benchmark.py for latency/throughput/memory measurement, benchmarks/quantize.py for bitsandbytes int8/int4 and torch.compile wrappers, benchmarks/compare.py for Rich table model comparison, (8) utils/s3.py for S3 shard listing, utils/hf.py for HF Hub model loading helpers, utils/viz.py for image grid visualization, (9) scripts/run_experiment.py CLI entry point and scripts/benchmark.py CLI entry point, (10) configs/base.yaml template config, (11) README.md with project overview and usage.
Execution mode: sequential
Total tickets: 10

COMPLETED BEFORE YOUR TASK:
  #1. [merged] Set up project structure and pyproject.toml
     Create the basic project structure with pyproject.toml containing all required dependencies. Set up directories: ml_experiments/ (main package), configs/, scripts/, tests/. Configure pyproject.toml with dependencies: torch, torchvision, diffusers, transformers, accelerate, modal, webdataset, boto3, pydantic, rich, bitsandbytes, torchmetrics, clean-fid, wandb. Include proper package metadata and entry points.

  #2. [completed] Implement core experiment base class
     Create ml_experiments/core/experiment.py with a base Experiment class. Include: Pydantic config loading from YAML, structured logging with rich, checkpointing to S3/local, metric tracking with wandb integration, abstract methods for train/eval/inference that subclasses must implement. Add proper error handling and validation.

  #3. [merged] Build Modal integration with GPU image and runner
     Create ml_experiments/core/modal_runner.py with Modal app definition. Include: CUDA+PyTorch+HuggingFace GPU image with all dependencies pre-installed, modal.Volume for HuggingFace model cache, generic run_experiment() function that takes experiment config dict and runs any experiment class on Modal GPUs (A10G/A100/H100). Handle GPU selection, volume mounting, and experiment instantiation.

  #4. [merged] Implement WebDataset S3 streaming utilities
     Create ml_experiments/core/data.py with WebDataset S3 integration. Include: get_dataset() function that takes dataset config with S3 path and returns streaming WebDataset pipeline, S3 presigned URL generation for streaming, standard image transforms pipeline, dataset registry for common datasets, error handling for network issues and malformed data.

  #5. [merged] Add metrics and profiling utilities
     Create ml_experiments/core/metrics.py with evaluation metrics and profiling tools. Include: FID score calculation using clean-fid, LPIPS and CLIP-score metrics, GPU memory profiling utilities, latency measurement with percentiles (p50/p95/p99), throughput calculation (images/sec), metric logging integration with the base Experiment class.

  #6. [merged] Create task-specific experiment stubs
     Create ml_experiments/tasks/ with stub experiment classes extending base Experiment: text_to_image.py (Stable Diffusion via diffusers), matting.py (image matting), segmentation.py (SAM integration), synthetic_data.py (Blender pipeline), vae.py (VAE training), text_encoder.py (CLIP/T5). Each should have proper config schemas and placeholder implementations.

  #7. [completed] Implement benchmarking utilities
     Create benchmarks/ module with three files: benchmark.py for latency/throughput/memory measurement with configurable runs and statistical analysis, quantize.py for bitsandbytes int8/int4 quantization and torch.compile optimization wrappers, compare.py for Rich table generation comparing multiple models side-by-side with metrics.

>>> YOUR TASK (#8): Add utility modules for S3, HuggingFace, and visualization

PLANNED AFTER YOUR TASK:
  #9. Create CLI entry points
     Create scripts/run_experiment.py as main CLI entry point that loads YAML configs and launches experiments locally or on Modal, and scripts/benchmark.py for model benchmarking with quantization options. Both should use rich for pretty CLI output, proper argument parsing with click/argparse, and integration with the core framework.

  #10. Add configuration templates and documentation
     Create configs/base.yaml with a comprehensive experiment config template including model settings, data paths, training hyperparameters, Modal GPU configuration. Create README.md with project overview, installation instructions, usage examples, and architecture explanation. Include examples for common workflows.

IMPORTANT: Focus ONLY on your task. Do not implement work that belongs to previous or future tickets. Keep your scope limited to exactly what your task describes.

================================================================================
TASK INSTRUCTIONS:
================================================================================

Create utils/ module with: s3.py for S3 WebDataset shard listing and presigned URL generation using boto3, hf.py for HuggingFace Hub model loading helpers with caching and error handling, viz.py for image grid visualization and tensorboard logging utilities. Include proper error handling and type hints.

================================================================================
DELIVERABLE - Complete these steps when done:
================================================================================

When you have completed the task:
1. git add -A && git commit -m "<descriptive message>"
2. git push -u origin HEAD
3. gh pr create --fill --base plan/scaffold-the-ml-experiments-framework-from-scratch-7b058357
4. gh pr merge --squash
5. Print the PR URL as the last line of output
These steps are MANDATORY. You must commit, push, create a PR, and merge it before finishing.
